{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [DeepSphere]: a spherical convolutional neural network\n",
    "[DeepSphere]: https://github.com/SwissDataScienceCenter/DeepSphere\n",
    "\n",
    "[Nathanaël Perraudin](https://perraudin.info), [Michaël Defferrard](http://deff.ch), Tomasz Kacprzak, Raphael Sgier\n",
    "\n",
    "# Demo: regression from muliple channels (spherical maps)\n",
    "\n",
    "This notebook shows how to regress (multiple) parameters from (multiple) input channels (spherical maps).\n",
    "It doesn't use real cosmological data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Run on CPU.\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import healpy as hp\n",
    "import tensorflow as tf\n",
    "\n",
    "from deepsphere import models, experiment_helper, plot\n",
    "from deepsphere.data import LabeledDataset\n",
    "from deepsphere.utils import HiddenPrints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (17, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = 'regression'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data loading\n",
    "\n",
    "Create a dataset by filtering random noise with different Gaussian filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nside = 32\n",
    "Nsamples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arcmin2rad(x):\n",
    "    return x / 60 / 360 * 2 * np.pi\n",
    "\n",
    "def gaussian_smoothing(sig, sigma, nest=True):\n",
    "    if nest:\n",
    "        sig = hp.reorder(sig, n2r=True)\n",
    "    smooth = hp.sphtfunc.smoothing(sig, sigma=sigma)\n",
    "    if nest:\n",
    "        smooth = hp.reorder(smooth, r2n=True)\n",
    "    smooth /= np.linalg.norm(smooth)\n",
    "    return smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(Nsamples, hp.nside2npix(Nside))\n",
    "t = np.random.rand(Nsamples, 2) + 0.5\n",
    "\n",
    "with HiddenPrints():\n",
    "    xs1 = np.array(list(map(gaussian_smoothing, x, t[:, 0] / Nside)))    \n",
    "    xs2 = np.array(list(map(gaussian_smoothing, x, t[:, 1] / Nside)))\n",
    "xs = np.stack((xs1, xs2), axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the spherical map before and after smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "cm = plt.cm.RdBu_r\n",
    "cm.set_under('w')\n",
    "hp.mollview(x[idx], title='Gaussian noise', nest=True, cmap=cm)\n",
    "hp.mollview(xs[idx, :, 0], title='Gaussian noise smoothed with sigma={:.2f}'.format(t[idx, 0]), nest=True, cmap=cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Data preparation\n",
    "\n",
    "* The regression target is the stddev of the Gaussian filters.\n",
    "* Split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and transform the data, i.e. extract features.\n",
    "x_raw = xs / np.mean(xs**2) # Apply some normalization (We do not want to affect the mean)\n",
    "\n",
    "# Create the label vector.\n",
    "labels = t - 0.5\n",
    "\n",
    "# Random train / test split.\n",
    "ntrain = int(0.8 * Nsamples)\n",
    "x_raw_train = x_raw[:ntrain]\n",
    "x_raw_test = x_raw[ntrain:]\n",
    "labels_train = labels[:ntrain]\n",
    "labels_test = labels[ntrain:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Regression using DeepSphere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['dir_name'] = EXP_NAME\n",
    "\n",
    "# Types of layers.\n",
    "params['conv'] = 'chebyshev5'  # Graph convolution: chebyshev5 or monomials.\n",
    "params['pool'] = 'max'  # Pooling: max or average.\n",
    "params['activation'] = 'relu'  # Non-linearity: relu, elu, leaky_relu, softmax, tanh, etc.\n",
    "params['statistics'] = None  # Statistics (for invariance): None, mean, var, meanvar, hist.\n",
    "\n",
    "# Architecture.\n",
    "params['statistics'] = 'mean'  # For predictions to be invariant to rotation.\n",
    "params['nsides'] = [Nside, Nside//2, Nside//4, Nside//8]  # Pooling: number of pixels per layer.\n",
    "params['F'] = [16, 32, 64]  # Graph convolutional layers: number of feature maps.\n",
    "params['M'] = [64, 2]  # Fully connected layers: output dimensionalities. Predict two parameters.\n",
    "params['input_channel'] = 2  # Two channels (spherical maps) per sample.\n",
    "params['loss'] = 'l2'  # Regression loss.\n",
    "params['K'] = [5] * len(params['F'])  # Polynomial orders.\n",
    "params['batch_norm'] = [True] * len(params['F'])  # Batch normalization.\n",
    "\n",
    "# Regularization.\n",
    "params['regularization'] = 0  # Amount of L2 regularization over the weights (will be divided by the number of weights).\n",
    "params['dropout'] = 1  # Percentage of neurons to keep.\n",
    "\n",
    "# Training.\n",
    "params['num_epochs'] = 10  # Number of passes through the training data.\n",
    "params['batch_size'] = 16  # Number of samples per training batch. Should be a power of 2 for greater speed.\n",
    "params['eval_frequency'] = 15  # Frequency of model evaluations during training (influence training time).\n",
    "params['scheduler'] = lambda step: 1e-4  # Constant learning rate.\n",
    "params['optimizer'] = lambda lr: tf.train.GradientDescentOptimizer(lr)\n",
    "#params['optimizer'] = lambda lr: tf.train.MomentumOptimizer(lr, momentum=0.5)\n",
    "#params['optimizer'] = lambda lr: tf.train.AdamOptimizer(lr, beta1=0.9, beta2=0.99, epsilon=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.deepsphere(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup before running again.\n",
    "shutil.rmtree('summaries/{}/'.format(EXP_NAME), ignore_errors=True)\n",
    "shutil.rmtree('checkpoints/{}/'.format(EXP_NAME), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = LabeledDataset(x_raw_train, labels_train)\n",
    "testing = LabeledDataset(x_raw_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_validation, loss_validation, loss_training, t_step = model.fit(training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot.plot_loss(loss_training, loss_validation, t_step, params['eval_frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(x_raw_test)\n",
    "pred_train = model.predict(x_raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(pred_test - labels_test)) / np.std(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(param=0):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(labels_train[:, param], pred_train[:, param], 'o', label='train')\n",
    "    ax.plot(labels_test[:, param], pred_test[:, param], 'o', label='test')\n",
    "    ax.plot([0, 1], [0, 1], linewidth=4, label='ground truth')\n",
    "    ax.set_title('Prediction of parameter {}'.format(param))\n",
    "    ax.legend()\n",
    "\n",
    "plot_prediction(0)\n",
    "plot_prediction(1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
